{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7539220,"sourceType":"datasetVersion","datasetId":4390101},{"sourceId":7542004,"sourceType":"datasetVersion","datasetId":4391920}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using LLM (CodeLlama-7b-Instruct-hf) for finding errors with code, giving completion score & find thought process of students\n## Reasons to use CodeLlama-7b-Instruct-hf:\n## •out of 7b,13b,34b,70b models ideally the most accurate results would be given by 70b parameters model & 34b model gives the best mix of speed and accuracy\n## •we use 7b model due to **computational resources limits** (20 gb GPU VRAM limit)","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom zipfile import ZipFile\nimport os\nfrom pathlib import Path\nimport io\nimport transformers\nimport torch\nimport pandas as pd\nimport re\nimport random\nfrom transformers import pipeline","metadata":{"id":"E18p36feRrb6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initializing Model and Pipeline\n## Initializing Text-Generation Pipeline ","metadata":{}},{"cell_type":"code","source":"model = \"codellama/CodeLlama-7b-Instruct-hf\"\n\n# Initializing the pipeline with the specific model\npipe = pipeline(\"text-generation\", model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making a function to convert each text file with timestamped code to a DataFrame with Relevant Details.\n## •Aids in easier prompting.\n## •Each Row is one Timestamp.\n## •In case there are more than 5 snapshots for a code, I have chosen only 5 snapshots for generating reports. (due to time limit and large number of reports to be made)","metadata":{}},{"cell_type":"code","source":"def parse_file_with_details_to_df(name,content):\n    blocks = content.split('.. activecode::')\n    timestamps, codes, languages, activecodes = [], [], [], []\n\n    for block in blocks[1:]:  # Skipping the first split as it's before the first activecode block\n        \n        # Extracting timestamp\n        timestamp_match = re.search(r':timestamp: ([\\d-]+\\s[\\d:]+)', block)\n        # Finding the start of the code block\n        code_start = block.find(':code:') + len(':code:')\n        # Extracting the coding language\n        language_match = re.search(r':language: (\\w+)', block)\n        # Extract the activecode block name\n        activecode_match = re.search(r'(\\w+)\\n', block)\n\n        if timestamp_match and code_start > len(':code:'):\n            timestamp = timestamp_match.group(1)\n            code = block[code_start:].strip()  # Extract code, stripping leading/trailing whitespace\n            language = language_match.group(1) if language_match else \"Unknown\"\n            activecode = activecode_match.group(1) if activecode_match else \"N/A\"\n\n            timestamps.append(timestamp)\n            codes.append(code)\n            languages.append(language)\n            activecodes.append(activecode)\n\n    # Creating a DataFrame with the extracted details\n    df = pd.DataFrame({\n        'Timestamp': pd.to_datetime(timestamps),\n        'Language': languages,\n        'ActiveCode': activecodes,\n        'Code': codes\n    })\n    \n    if len(df) > 5:\n        rows_to_keep = [0]  # First row\n        middle_indices = random.sample(range(1, len(df) - 1), k=3)\n        rows_to_keep.extend(sorted(middle_indices))\n        rows_to_keep.append(len(df)-1)  # Last row\n        df = df.iloc[rows_to_keep]\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **NEXT FUNCTION PERFORMS THE FOLLOWING :** \n\n# THEME/QUESTION GUESSING (to give LLM idea on how to evaluate student codes)\n## •Since the questions corresponding solutions are not available, I am using the model to guess the question based on the solution and then usimg it for evaluation\n## •If the Questions were made available (since they are already present in a database), the report would be more robust\n\n## Initializing 'History' : Passes last few inference outputs to the LLM as part of prompt to give idea about sequential codes\n## Initializing 'flow' : Will be converted to a Report in .txt format\n\n# ERROR FINDING ON THE BASIS OF THE QUESTION\n## •We evaluate the code and students performance\n## •For now, we allow the model to give report in its own way since no report format is mentioned\n## •Ideally, if a given format is decided on, we can fine-tune the LLM or use multi shot prompting to give outputs in desired format\n\n# Making an automated Report & Saving in a Reports Folder","metadata":{}},{"cell_type":"code","source":"def process_reports(name,content):\n    \n    #Using function to convert .txt file to a dataframe for easier processing\n    df = parse_file_with_details_to_df(name,content)\n    \n    #prompting for Question Generation\n    prompt = '''<s>[INST]\n    {{ '''+ df['Code'][0] + '''\n       find the question this code is trying to solve:\n\n    }} [/INST]'''\n\n\n    result = pipe(prompt, max_length=1000, return_full_text=False)\n    \n    #Saving the Question generated as theme of the problem\n    theme = result[0]['generated_text']\n    \n\n    history_list = []\n\n    history = str()\n    \n    #Documenting the whole coding process of the student in 'flow'\n    flow = \"CODE TOPIC : \" + str(df['ActiveCode'][0])  + \"\\nQUESTION : \" + theme + \"\\n \\n\"\n    \n    #Looping through the different Timestamps in the DataFrame to find and document progress of Student\n    for index, row in df.iterrows():\n        timestamp = row ['Timestamp']\n        Language = row ['Language']\n        code = row['Code']\n        timestep = str(index)\n\n        if (len(history_list))>2:\n            history = ' '.join(history_list[-2:])\n        else :\n            history = ' '.join(history_list)\n\n        if index==0:\n            prompt = '''<s><<SYS>>Help find any issues with the students thinking process \\n<</SYS>>[INST] Given Question :''' + theme +'''Language:'''+ Language + '''Timestep :''' + timestep +'''\\n''' + code + '''explain errors in the Student's new code trying to solve the given question and give him an overall score out of 100. I dont need corrected code:}} [/INST]'''\n        else:\n            prompt = '''<s><<SYS>>Help find any issues with the students thinking process \\n<</SYS>>[INST] Given Question :''' + theme +'''Language:'''+ Language + '''Timestep :''' + timestep +''' here is a list of errors the student was previously facing at different given timestamps: }} [/INST] {{''' + history + '''}} </s><s>[INST] {{Does this new code resolve any of the previous issues :''' + code + '''explain errors in the Student's new code trying to solve the given question and give him an overall score out of 100. I dont need corrected code:}} [/INST]'''\n            \n        #Generating text/code based on the solution prompt\n        result = pipe(prompt, max_length=5000, return_full_text=False, temperature = 0.7, top_p = 0.95, top_k = 250, do_sample = True)\n\n        generated = result[0]['generated_text']\n        \n        # Extracting the generated Message and putting it into a meaningful form\n        temporary_history = \"Timestep :\" + timestep + \"\\nIssues : \\n\" + generated\n        flow = flow + \"\\n\" + \"Timestamp :\" + str(timestamp)+ \"\\nCode : \\n\" + code + \"\\nIssues : \\n\" + generated\n\n        history_list.append(temporary_history)\n        \n    #Defining Path for savin Reports\n    report_path = \"Reports\" + \"/\".join(name.split(\"/\")[1:])\n    report_dir = os.path.dirname(report_path)\n        \n    # Creating necessary directories for saving reports\n    if not os.path.exists(report_dir):\n        os.makedirs(report_dir, exist_ok=True)\n        \n    # Writing the processed reports\n    with open(report_path, 'w') as file:\n        file.write(flow)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the path to the directory containing the .txt files\ndirectory_path = '/kaggle/input/psychic-invention/psychic-invention-main'\n\n\n#Function to initiate processing of all .txt files Given\ndef process_txt_files_in_directory(directory_path):\n    \n    # Iterating over to find and process the .txt files in the directory\n    \n    for root, dirs, files in os.walk(directory_path):\n        \n        for file in files:\n            \n            if file.endswith('.txt'):\n                file_path = Path(root) / file\n                \n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    # Calling Function to start processing the extracted text fiie\n                    process_reports(file_path.as_posix(), content)\n                    print(f\"Processed file: {file_path}\")  # Placeholder for actual processing\n\n# Calling the function to process .txt files in the specified directory\nprocess_txt_files_in_directory(directory_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r REPORTS.zip /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom IPython.display import FileLink\nFileLink(r'REPORTS.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}